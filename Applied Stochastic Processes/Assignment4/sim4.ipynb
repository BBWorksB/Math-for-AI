{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial parameters for components\n",
    "pi1, mu1, sigma1_squared = 0.5, 2.0000137, 0.66674\n",
    "pi2, mu2, sigma2_squared = 0.5, 6.99999, 0.66674\n",
    "\n",
    "# Storage for data points and responsibilities\n",
    "x_values = []\n",
    "responsibilities = []\n",
    "\n",
    "def gaussian_pdf(x, mu, sigma_squared):\n",
    "    \"\"\"Calculate the Gaussian PDF.\"\"\"\n",
    "    return (1 / np.sqrt(2 * np.pi * sigma_squared)) * np.exp(-((x - mu) ** 2) / (2 * sigma_squared))\n",
    "\n",
    "def calculate_responsibilities(x, pi1, mu1, sigma1_squared, pi2, mu2, sigma2_squared):\n",
    "    \"\"\"Calculate responsibilities γ_i1 and γ_i2 for a data point x and print PDFs.\"\"\"\n",
    "    p_x_given_c1 = gaussian_pdf(x, mu1, sigma1_squared)\n",
    "    p_x_given_c2 = gaussian_pdf(x, mu2, sigma2_squared)\n",
    "    \n",
    "    total = pi1 * p_x_given_c1 + pi2 * p_x_given_c2\n",
    "    gamma_i1 = pi1 * p_x_given_c1 / total\n",
    "    gamma_i2 = pi2 * p_x_given_c2 / total\n",
    "    \n",
    "    # Store x and its responsibilities\n",
    "    x_values.append(x)\n",
    "    responsibilities.append((gamma_i1, gamma_i2))\n",
    "    \n",
    "    # Print PDF values and responsibilities\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"PDF for component 1: {p_x_given_c1}\")\n",
    "    print(f\"PDF for component 2: {p_x_given_c2}\")\n",
    "    print(f\"Responsibility γ_i1: {gamma_i1}, γ_i2: {gamma_i2}\")\n",
    "    \n",
    "    return gamma_i1, gamma_i2\n",
    "\n",
    "def maximization():\n",
    "    \"\"\"Perform maximization step to update model parameters and print updated values.\"\"\"\n",
    "    global pi1, mu1, sigma1_squared, pi2, mu2, sigma2_squared\n",
    "    \n",
    "    N = len(x_values)\n",
    "    # Sum of responsibilities for each component\n",
    "    sum_gamma1 = sum(g[0] for g in responsibilities)\n",
    "    sum_gamma2 = sum(g[1] for g in responsibilities)\n",
    "    \n",
    "    # Update mixing coefficients\n",
    "    pi1 = sum_gamma1 / N\n",
    "    pi2 = sum_gamma2 / N\n",
    "    \n",
    "    # Update means\n",
    "    mu1 = sum(g[0] * x for g, x in zip(responsibilities, x_values)) / sum_gamma1\n",
    "    mu2 = sum(g[1] * x for g, x in zip(responsibilities, x_values)) / sum_gamma2\n",
    "    \n",
    "    # Update variances\n",
    "    sigma1_squared = sum(g[0] * (x - mu1) ** 2 for g, x in zip(responsibilities, x_values)) / sum_gamma1\n",
    "    sigma2_squared = sum(g[1] * (x - mu2) ** 2 for g, x in zip(responsibilities, x_values)) / sum_gamma2\n",
    "    \n",
    "    # Print updated parameters\n",
    "    print(\"\\nUpdated parameters after maximization:\")\n",
    "    print(f\"pi1 = {pi1}, mu1 = {mu1}, sigma1_squared = {sigma1_squared}\")\n",
    "    print(f\"pi2 = {pi2}, mu2 = {mu2}, sigma2_squared = {sigma2_squared}\")\n",
    "\n",
    "# Example usage of expectation and maximization\n",
    "x_samples = [1,2,3,6,7,8]  # Example data points\n",
    "\n",
    "# Expectation step for each data point\n",
    "for x in x_samples:\n",
    "    calculate_responsibilities(x, pi1, mu1, sigma1_squared, pi2, mu2, sigma2_squared)\n",
    "\n",
    "# Maximization step\n",
    "maximization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters for a 3-component GMM\n",
    "n_components = 3\n",
    "n_features = iris_scaler.shape[1]\n",
    "n_samples = iris_scaler.shape[0]\n",
    "\n",
    "# Randomly initialize means, covariances, and mixing coefficients\n",
    "np.random.seed(0)\n",
    "means = np.random.rand(n_components, n_features)\n",
    "covariances = np.array([np.eye(n_features) for _ in range(n_components)])\n",
    "mixing_coeffs = np.full(n_components, 1 / n_components)  # Equal mixing coefficients\n",
    "\n",
    "\n",
    "\n",
    "def e_step(X, means, covariances, mixing_coeffs):\n",
    "    responsibilities = np.zeros((n_samples, n_components))\n",
    "\n",
    "    for i in range(n_components):\n",
    "        # Calculate the probability density for each Gaussian component\n",
    "        pdf = multivariate_normal.pdf(X, mean=means[i], cov=covariances[i])\n",
    "        responsibilities[:, i] = mixing_coeffs[i] * pdf\n",
    "\n",
    "    # Normalize to get probabilities (responsibilities should sum to 1 across components)\n",
    "    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "    return responsibilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities):\n",
    "    # Update mixing coefficients\n",
    "    Nk = responsibilities.sum(axis=0)\n",
    "    mixing_coeffs = Nk / n_samples\n",
    "    \n",
    "    # Update means\n",
    "    means = np.dot(responsibilities.T, X) / Nk[:, np.newaxis]\n",
    "\n",
    "    # Update covariances\n",
    "    covariances = []\n",
    "    for i in range(n_components):\n",
    "        X_centered = X - means[i]\n",
    "        cov = (responsibilities[:, i, np.newaxis] * X_centered).T @ X_centered / Nk[i]\n",
    "        covariances.append(cov)\n",
    "    \n",
    "    return means, np.array(covariances), mixing_coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "max_iters = 100\n",
    "tolerance = 1e-4  # Convergence tolerance\n",
    "log_likelihoods = []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    # E-step\n",
    "    responsibilities = e_step(iris_scaler, means, covariances, mixing_coeffs)\n",
    "    \n",
    "    # M-step\n",
    "    means, covariances, mixing_coeffs = m_step(iris_scaler, responsibilities)\n",
    "    \n",
    "    # Log likelihood (for convergence check)\n",
    "    log_likelihood = np.sum(np.log(responsibilities.sum(axis=1)))\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "\n",
    "    # Check for convergence\n",
    "    if iteration > 0 and abs(log_likelihood - log_likelihoods[-2]) < tolerance:\n",
    "        print(f\"Converged at iteration {iteration}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "print(\"Final Means:\\n\", means)\n",
    "print(\"Final Covariances:\\n\", covariances)\n",
    "print(\"Final Mixing Coefficients:\\n\", mixing_coeffs)\n",
    "\n",
    "# Reduce data to two dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(iris_scaler)\n",
    "\n",
    "# Assign each point to the component with the highest responsibility\n",
    "cluster_assignments = np.argmax(responsibilities, axis=1)\n",
    "\n",
    "# Plot the data points and color by cluster assignment\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_components):\n",
    "    plt.scatter(X_pca[cluster_assignments == i, 0], X_pca[cluster_assignments == i, 1], label=f'Cluster {i+1}')\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend()\n",
    "plt.title(\"GMM Clustering on Iris Dataset (PCA-reduced)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
